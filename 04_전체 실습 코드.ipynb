{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0TY4VoHqGyz"
   },
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AZnN8_4jpHF2"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ID5VoH1Fp8BB",
    "outputId": "26bab963-6ae0-4a87-b2f1-b00a2aa15e09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Q4UtBpVap_x5",
    "outputId": "2ceb4bdc-c1d0-4d86-d459-895a4f56e11e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/.shortcut-targets-by-id/12KHC7G8PoQHHvGxvqV6nXOgUgwqOIO9O/ESAA 데이터 분석 프로젝트 (가짜 뉴스 분류)/Data'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경로 설정       \n",
    "os.chdir('/content/drive/My Drive/ESAA 데이터 분석 프로젝트 (가짜 뉴스 분류)/Data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "nHFympeEqAdq",
    "outputId": "568b2613-35eb-4405-ebb9-6d7825d0bb81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_id</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>ord</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEWS02580</td>\n",
       "      <td>20200605</td>\n",
       "      <td>[마감]코스닥 기관 678억 순매도</td>\n",
       "      <td>[이데일리 MARKETPOINT]15:32 현재 코스닥 기관 678억 순매도</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEWS02580</td>\n",
       "      <td>20200605</td>\n",
       "      <td>[마감]코스닥 기관 678억 순매도</td>\n",
       "      <td>\"실적기반\" 저가에 매집해야 할 8월 급등유망주 TOP 5 전격공개</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEWS02580</td>\n",
       "      <td>20200605</td>\n",
       "      <td>[마감]코스닥 기관 678억 순매도</td>\n",
       "      <td>하이스탁론, 선취수수료 없는 월 0.4% 최저금리 상품 출시</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEWS02580</td>\n",
       "      <td>20200605</td>\n",
       "      <td>[마감]코스닥 기관 678억 순매도</td>\n",
       "      <td>종합 경제정보 미디어 이데일리 - 무단전재 &amp; 재배포 금지</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEWS09727</td>\n",
       "      <td>20200626</td>\n",
       "      <td>롯데·공영 등 7개 TV 홈쇼핑들, 동행세일 동참</td>\n",
       "      <td>전국적인 소비 붐 조성에 기여할 예정</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        n_id      date  ... ord info\n",
       "0  NEWS02580  20200605  ...   1    0\n",
       "1  NEWS02580  20200605  ...   2    1\n",
       "2  NEWS02580  20200605  ...   3    1\n",
       "3  NEWS02580  20200605  ...   4    0\n",
       "4  NEWS09727  20200626  ...   1    0\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"news_train.csv\")\n",
    "test = pd.read_csv(\"news_test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a7Jha5gqQ8l"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1Ij53IKqNer"
   },
   "outputs": [],
   "source": [
    "# 그래프 한글 폰트 설정\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "!apt -qq -y install fonts-nanum\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "#폰트 선명하게 보이게 하기 위해\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "## Test\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.text(0.3, 0.3, '한글 폰트 테스트', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pctmun_lqPxT"
   },
   "outputs": [],
   "source": [
    "# 기본 라이브러리\n",
    "import numpy as np\n",
    "from datetime import * \n",
    "import math\n",
    "\n",
    "# 시각화 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpDkXNE-qwOW"
   },
   "outputs": [],
   "source": [
    "#결측치 확인\n",
    "np.sum(train['date'].isna()), np.sum(train['title'].isna()), np.sum(train['content'].isna()), np.sum(train['ord'].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyHuTvoMtzNV"
   },
   "outputs": [],
   "source": [
    "real=train[train['info']==0] # 진짜 \n",
    "fake=train[train['info']==1] # 가짜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XD06HHKnsBdz"
   },
   "source": [
    "진짜 뉴스와 가짜 뉴스의 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nLdSFnFq3bd"
   },
   "outputs": [],
   "source": [
    "print(\"진짜 뉴스 개수 : {}\".format(len(train.loc[train['info']==0])))\n",
    "print(\"가짜 뉴스 개수 : {}\".format(len(train.loc[train['info']==1])))\n",
    "print(\"진짜 뉴스 비율 : {}%\".format(round((len(train.loc[train['info']==0])) / len(train.index) * 100, 3)))\n",
    "print(\"가짜 뉴스 비율 : {}%\".format(round((len(train.loc[train['info']==1])) / len(train.index) * 100, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI0ChCrLq_Sl"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=train, x=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jcs0mri_rAJ9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(data=train, x='date', hue='info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMvv7reXsFZB"
   },
   "source": [
    "반복 등장 횟수 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZGQNs3BrC6l"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.distplot(repe_fake['n_id'], label=\"가짜뉴스의 반복 등장 횟수\")\n",
    "sns.distplot(repe_real['n_id'], label=\"진짜뉴스의 반복 등장 횟수\")\n",
    "plt.xlim(0, 250)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1XLY1Dnr8Of"
   },
   "source": [
    "반복 등장하는 단어의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJnnwOKerzn0"
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "!pip install matplotlib\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FF01E5_Jr6-E"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PH4GPLEgsOK9"
   },
   "outputs": [],
   "source": [
    "def make_wordcloud(data, word_count):\n",
    "    twitter = Twitter()\n",
    "    sentences_tag = []\n",
    "    #형태소 분석하여 리스트에 넣기\n",
    "    for sentence in data:\n",
    "        morph = twitter.pos(sentence)\n",
    "        sentences_tag.append(morph)\n",
    "        \n",
    " \n",
    "    noun_adj_list = []\n",
    "    #명사와 형용사만 구분하여 이스트에 넣기\n",
    "    for sentence1 in sentences_tag:\n",
    "        for word, tag in sentence1:\n",
    "            if tag in ['Noun', 'Adjective']:\n",
    "                noun_adj_list.append(word)\n",
    "    \n",
    "    #형태소별 count\n",
    "    counts = Counter(noun_adj_list)\n",
    "    tags = counts.most_common(word_count)\n",
    "    print(tags)\n",
    " \n",
    "    #wordCloud생성\n",
    "    #한글깨지는 문제 해결하기위해 font_path 지정\n",
    "    wc = WordCloud(font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf', background_color='white', width=800, height=600)\n",
    "    print(dict(tags))\n",
    "    cloud = wc.generate_from_frequencies(dict(tags))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(cloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGohTTuEsTfQ"
   },
   "outputs": [],
   "source": [
    "make_wordcloud(fake['content'],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcVbgY5tsWHL"
   },
   "outputs": [],
   "source": [
    "make_wordcloud(real['content'],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SX61MX_soP2"
   },
   "outputs": [],
   "source": [
    "make_wordcloud(train['content'],100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4OCETPBuIOy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBod19lYuZld"
   },
   "source": [
    "## Tokenizing 및 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXEIHWlkxTKR"
   },
   "source": [
    "Twitter, Komoran, Hannanum 등의 형태소 분석기(Pos Tagger)들이 존재. \n",
    "속도와 정확도 면에서 차이가 있으며 주로 Mecab 분석기를 이용함. \n",
    "\n",
    "Mecab: 굉장히 속도가 빠르면서도 좋은 분석 결과를 보여준다.\n",
    "\n",
    "Komoran: 댓글과 같이 정제되지 않은 글에 대해서 먼저 사용해보면 좋다.(오탈자를 어느정도 고려해준다.)\n",
    "\n",
    "Kkma: 분석 시간이 오래걸리기 때문에 잘 이용하지 않게 된다.\n",
    "\n",
    "Okt: 품사 태깅 결과를 Noun, Verb등 알아보기 쉽게 반환해준다.\n",
    "\n",
    "khaiii: 카카오에서 가장 최근에 공개한 분석기, 성능이 좋다고 알려져 있으며 다양한 실험이 필요하다.\n",
    "\n",
    "형태소 분석기 비교 https://iostream.tistory.com/144 \n",
    "https://mr-doosun.tistory.com/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_J80etQkFweP"
   },
   "outputs": [],
   "source": [
    "#텍스트 예시\n",
    "text = u\"\"\"ESAA 2조 프로젝트 파이팅 힘내요\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LfAYMixHxFV"
   },
   "outputs": [],
   "source": [
    "!pip3 install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yT2NLwW94uy9",
    "outputId": "c9bb3cba-f66c-44fc-9995-894c08688bf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[('ESAA', 'f')]], [[('2', 'nnc'), ('조', 'nbu')], [('2', 'nnc'), ('조', 'nnc')]], [[('프로젝트', 'ncn')]], [[('파이팅', 'ncn')]], [[('힘내', 'pvg'), ('어요', 'ef')], [('힘내', 'pvg'), ('어', 'ef'), ('요', 'jxf')]]]\n",
      "['ESAA', '2조', '프로젝트', '파이팅', '힘내', '어요']\n",
      "['2조', '프로젝트', '파이팅']\n",
      "[('ESAA', 'F'), ('2조', 'N'), ('프로젝트', 'N'), ('파이팅', 'N'), ('힘내', 'P'), ('어요', 'E')]\n"
     ]
    }
   ],
   "source": [
    "#Hannanum\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "\n",
    "print(hannanum.analyze(text)) #다양한 형태로 변환(사전검색, 분류되지 않은 용어 등)\n",
    "print(hannanum.morphs(text)) #텍스트에서 형태소 반환\n",
    "print(hannanum.nouns(text))#텍스트에서 명사 반환\n",
    "print(hannanum.pos(text))#텍스트에서 품사 정보 부착하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88o1gHs55ZGn",
    "outputId": "e5deda21-ecb0-41d0-fa79-3a8f9ab45668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ESAA', '2', '조', '프로젝트', '파이팅', '힘내', '요']\n",
      "['프로젝트', '파이팅']\n",
      "[('ESAA', 'SL'), ('2', 'SN'), ('조', 'NR'), ('프로젝트', 'NNP'), ('파이팅', 'NNP'), ('힘내', 'VV'), ('요', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "#Komoran\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "print(komoran.morphs(text)) #텍스트에서 형태소 반환\n",
    "print(komoran.nouns(text))#텍스트에서 명사 반환\n",
    "print(komoran.pos(text))#텍스트에서 품사 정보 부착하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJ89wf6W52YG",
    "outputId": "87ea4a8e-c992-424e-d8d2-243b80c28038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ESAA', '2조', '프로젝트', '파이팅', '힘내요']\n",
      "['프로젝트', '파이팅']\n",
      "[('ESAA', 'Alpha'), ('2조', 'Number'), ('프로젝트', 'Noun'), ('파이팅', 'Noun'), ('힘내요', 'Verb')]\n",
      "['ESAA', 'ESAA 2조', 'ESAA 2조 프로젝트', 'ESAA 2조 프로젝트 파이팅', '2조', '프로젝트', '파이팅']\n"
     ]
    }
   ],
   "source": [
    "#Okt (과거 Twitter 분석기)\n",
    "#from konlpy.tag import Twitter\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "#twitter=Twitter()\n",
    "okt = Okt()\n",
    "\n",
    "print(okt.morphs(text)) #텍스트에서 형태소 반환\n",
    "print(okt.nouns(text)) #텍스트에서 명사 반환\n",
    "print(okt.pos(text)) #텍스트에서 품사 정보 부착하여 반환\n",
    "print(okt.phrases(text)) #텍스트에서 어절을 뽑아냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bqp9MpZFIpB"
   },
   "source": [
    "#### MeCab 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sgbAEonxSFH"
   },
   "outputs": [],
   "source": [
    "#Mecab\n",
    "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "\n",
    "#출처: https://somjang.tistory.com/entry/Google-Colab에서-Mecab-koMecab-ko-dic-쉽게-사용하기 [솜씨좋은장씨]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izs9_hSR2YTz"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZP2QP-Y2jW3"
   },
   "outputs": [],
   "source": [
    " cd Mecab-ko-for-Google-Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "navYr_fB2m0i"
   },
   "outputs": [],
   "source": [
    "! bash install_mecab-ko_on_colab190912.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq3Y2L8z3fh7"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab \n",
    "mecab = Mecab()\n",
    "\n",
    "print(mecab.nouns) #텍스트에서 명사 반환\n",
    "print(mecab.morphs(text)) #텍스트에서 형태소 반환\n",
    "print(mecab.pos(text)) #텍스트에서 품사 정보 부착하여 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60S_scX1_W7c"
   },
   "source": [
    "#### khaiii(카카오) 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPiJPzNU8JeG"
   },
   "outputs": [],
   "source": [
    "#Khaiii\n",
    "!git clone https://github.com/kakao/khaiii.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtLeVq2g_Eko"
   },
   "outputs": [],
   "source": [
    "!pip install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulfulcV8_Gen"
   },
   "outputs": [],
   "source": [
    "!mkdir build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Itlz1jw_I_Q"
   },
   "outputs": [],
   "source": [
    "!cd build && cmake /content/khaiii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FRW3RBE_KX_"
   },
   "outputs": [],
   "source": [
    "!cd /content/build/ && make all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTWHl8QF_MlV"
   },
   "outputs": [],
   "source": [
    "!cd /content/build/ && make resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7mG0jDu_OKS"
   },
   "outputs": [],
   "source": [
    "!cd /content/build && make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGKSzZ-2_Qga"
   },
   "outputs": [],
   "source": [
    "!cd /content/build && make package_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXu2WeIK_SUh"
   },
   "outputs": [],
   "source": [
    "!pip install /content/build/package_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMbRHm9P9gJO"
   },
   "outputs": [],
   "source": [
    "from khaiii import KhaiiiApi\n",
    "api = KhaiiiApi()\n",
    "for word in api.analyze(text):\n",
    "    print(word)\n",
    "\n",
    "#출처: https://banana-media-lab.tistory.com/entry/colab에서-khaiii-형분석기-설치해서-사용하기 [Banana Media Lab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds4vc8l4wgHT"
   },
   "source": [
    "### 스톱워드 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgX7l-nJwujH"
   },
   "source": [
    "Mecab 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7t6lg-ZXww71"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "import tqdm\n",
    "mecab  = Mecab()\n",
    "\n",
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "    tokenizer = Mecab()\n",
    "    token_list = []\n",
    "    \n",
    "    for text in tqdm.tqdm(text_list):\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text.lower()) \n",
    "        # 한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
    "        token = tokenizer.morphs(txt) # 형태소 분석\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float] \n",
    "        #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
    "        token_list.append(token)\n",
    "        \n",
    "    return token_list, tokenizer\n",
    "\n",
    "train['new_article'], mecab = text_preprocessing(train['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbliFtYtwqG7"
   },
   "source": [
    "Komoran 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTHE0z-kwlAc"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import re\n",
    "import tqdm\n",
    "komoran  = Komoran()\n",
    "\n",
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "    tokenizer = Komoran()\n",
    "    token_list = []\n",
    "    \n",
    "    for text in tqdm.tqdm(text_list):\n",
    "      try:\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text.lower()) \n",
    "        # 한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
    "        token = tokenizer.morphs(txt) # 형태소 분석\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float] \n",
    "        #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
    "        token_list.append(token)\n",
    "      except:\n",
    "        token_list.append(None)\n",
    "    return token_list, tokenizer\n",
    "\n",
    "train['new_article'], komoran = text_preprocessing(train['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBXo9MwUxAdY"
   },
   "outputs": [],
   "source": [
    "train['new_article'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqOnN12ExCVL"
   },
   "outputs": [],
   "source": [
    "train[train['new_article'].isnull() & train['info']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDn1iMRIxC4J"
   },
   "outputs": [],
   "source": [
    "train[(train['new_article'].isnull()) & (train['info']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-UPYT3BxITp"
   },
   "outputs": [],
   "source": [
    "# train으로 학습시킨 형태소 분석기 test에 적용\n",
    "stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "token_list2 = []\n",
    "for text in tqdm.tqdm(test['content']):\n",
    "  try:\n",
    "    txt2 = re.sub('[^가-힣a-z]', ' ', text.lower())\n",
    "    token2 = komoran.morphs(txt2)\n",
    "    token2 = [t for t in token2 if t not in stopwords or type(t) != float]\n",
    "    token_list2.append(token2)\n",
    "  except:\n",
    "    token_list2.append(None)\n",
    "test['new_article'] = token_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dPzx5ohxL0-"
   },
   "outputs": [],
   "source": [
    "test[(test['new_article'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCOII2cCxMWm"
   },
   "outputs": [],
   "source": [
    "test[(test['new_article'].isnull()) & (test['content']==']]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_hQrPct0cHe"
   },
   "outputs": [],
   "source": [
    "train_notnull = train[train['new_article'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idqVuAFC7I3P"
   },
   "source": [
    "새로운 stopword 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GIaBd6j65dZ"
   },
   "outputs": [],
   "source": [
    "stopword = pd.read_csv(\"stopwords.txt\", sep='\\t',names=['word', 'pos', 'val'], header=None)\n",
    "stopword_new = stopword['word'] # 바꿔서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdQFW3iVzG1C"
   },
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YLN1RDCzcgf"
   },
   "source": [
    "Vectorizer로 text2sequence 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_TSLFYyxRo2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def text2sequence(train_text, max_len=1000):\n",
    "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
    "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
    "    train_X_seq = tokenizer.texts_to_sequences(train_text) \n",
    "    #각 토큰들에 정수 부여\n",
    "    vocab_size = len(tokenizer.word_index) + 1 \n",
    "    #모델에 알려줄 vocabulary의 크기 계산\n",
    "    print('vocab_size : ', vocab_size)\n",
    "    X_train = pad_sequences(train_X_seq, maxlen = max_len) \n",
    "    #설정한 문장의 최대 길이만큼 padding\n",
    "    \n",
    "    return X_train, vocab_size, tokenizer\n",
    "\n",
    "train_y = train_notnull['info']\n",
    "train_X, vocab_size, vectorizer = text2sequence(train_notnull['new_article'], max_len = 100)\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrxpGvzdzoRo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('train_X',train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm5-btguzrBI"
   },
   "outputs": [],
   "source": [
    "test_notnull = test[test['new_article'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9RFdjhLzs1T"
   },
   "outputs": [],
   "source": [
    "# train으로 학습시킨 vectorizer test에도 적용\n",
    "test_X_seq = vectorizer.texts_to_sequences(test_notnull['new_article'])\n",
    "test_X = pad_sequences(test_X_seq, maxlen = 100)\n",
    "#np.save('test_X', test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fbqROErzzOS"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL9_-olsz4ec"
   },
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtyRIn9mzu78"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHeC0kN70F0H"
   },
   "outputs": [],
   "source": [
    "#경로 다시 바꿔주기\n",
    "os.chdir('/content/drive/My Drive/[데이콘] 한국어 문서 추출요약 AI 경진대회/Data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzL0KVfT0H_j"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "embedding_matrix = np.zeros((vocab_size, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4D_WAUG0KJC"
   },
   "outputs": [],
   "source": [
    "for index, word in enumerate(vectorizer.word_index):\n",
    "    if word in word2vec:\n",
    "        embedding_vector = word2vec[word] \n",
    "        embedding_matrix[index] = embedding_vector \n",
    "    else:\n",
    "        print(\"word2vec에 없는 단어입니다.\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-JRKG4f0MBF"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf9vtY6A8v1F"
   },
   "source": [
    "#### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8--d-sq87wB"
   },
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "from gensim.models.wrappers import FastText \n",
    "fastText = FastText.load_fasttext_format('fasttext.bin')\n",
    "embedding_matrix = np.zeros((vocab_size, 100)) #300차원의 임베딩 매트릭스 생성\n",
    "for index, word in enumerate(vectorizer.word_index): \n",
    "  #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
    "    if word in fastText: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
    "        embedding_vector = fastText[word] #해당 토큰에 해당하는 vector를 불러오고\n",
    "        embedding_matrix[index] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5k_MRXE8_FV"
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8RCxV7q0SkH"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqehXamT3Mtc"
   },
   "source": [
    "### Auto ML -전처리X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7Z0LJo03tgL"
   },
   "source": [
    "정확도 : 0.9458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIG_hTqG0V17"
   },
   "outputs": [],
   "source": [
    "!pip install autokeras\n",
    "!pip install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc4\n",
    "!pip install autokeras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmt9dBun3W4K"
   },
   "outputs": [],
   "source": [
    "X = train.content.values\n",
    "Y = train['info'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9UP8bxM3Y5z"
   },
   "outputs": [],
   "source": [
    "input_node = ak.TextInput()\n",
    "output_node = ak.TextToIntSequence()(input_node)\n",
    "output_node = ak.Embedding()(output_node)\n",
    "output_node = ak.ConvBlock(separable=True)(output_node)\n",
    "output_node = ak.ClassificationHead()(output_node)\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node,\n",
    "    outputs=output_node,\n",
    "    overwrite=True,\n",
    "    max_trials=5\n",
    "    )\n",
    "clf.fit(X, Y, epochs=5)\n",
    "model = clf.export_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bym8a0A93eQ-"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "pred_test = model.predict(test.content.values)\n",
    "sample_submission.loc[:,'info'] = np.where(pred_test> 0.5, 1,0).reshape(-1)\n",
    "sample_submission.loc[:,[\"id\",\"info\"]].to_csv(\"submission_AutoML1\n",
    ".csv\", index = False)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb1_ZJMK3glL"
   },
   "source": [
    "### Auto ML - vectorizer까지 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCFH2c0c3zhL"
   },
   "source": [
    "정확도 : 0.8497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CToabShi36lh"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "input_node = ak.StructuredDataInput()\n",
    "#output_node = ak.TextToIntSequence()(input_node)\n",
    "#output_node = ak.Embedding()(output_node)\n",
    "#output_node = ak.ConvBlock(separable=True)(output_node)\n",
    "output_node = ak.ClassificationHead()\n",
    "clf = ak.AutoModel(\n",
    "    inputs=input_node, \n",
    "    outputs=output_node, \n",
    "    max_trials = 5, \n",
    "    overwrite=True)\n",
    "clf.fit(train_X, train_y, epochs=5)\n",
    "model = clf.export_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbeU5CH139LM"
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(test_X)\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission.loc[:,'info'] = np.where(pred_test>0.5,1,0).reshape(-1)\n",
    "submission.loc[:,['id','info']].to_csv(\"es_submission.csv\", index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A74jOBJ53_9H"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HQdbfNF4aO0"
   },
   "source": [
    "정확도 : 0.9626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9-DYXmn4Ddc"
   },
   "outputs": [],
   "source": [
    "X_data = train['content']\n",
    "y_train = train['info']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = total_cnt - rare_cnt + 1)\n",
    "tokenizer.fit_on_texts(X_data) # 5169개의 행을 가진 X의 각 행에 토큰화를 수행\n",
    "sequences = tokenizer.texts_to_sequences(X_data) # 단어를 숫자값, 인덱스로 변환하여 저장\n",
    "word_to_index = tokenizer.word_index\n",
    "vocab_size = len(word_to_index) + 1\n",
    "print('단어 집합의 크기: {}'.format((vocab_size)))\n",
    "\n",
    "X_data = sequences\n",
    "print('메일의 최대 길이 : %d' % max(len(l) for l in X_data))\n",
    "print('메일의 평균 길이 : %f' % (sum(map(len, X_data))/len(X_data)))\n",
    "\n",
    "max_len = 575 #메일의 최대 길이\n",
    "\n",
    "# 전체 데이터셋의 길이는 max_len으로 맞춥니다.\n",
    "X_train = pad_sequences(X_data, maxlen = max_len)\n",
    "print(\"훈련 데이터의 크기(shape): \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6V1kmxV4Pht"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32)) # 임베딩 벡터의 차원은 32\n",
    "model.add(SimpleRNN(32)) # RNN 셀의 hidden_size는 32\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NLvrGmt4Szz"
   },
   "outputs": [],
   "source": [
    "testX_data = test['content']\n",
    "sequences = tokenizer.texts_to_sequences(testX_data) # 단어를 숫자값, 인덱스로 변환하여 저장\n",
    "X_data = sequences\n",
    "print('메일의 최대 길이 : %d' % max(len(l) for l in X_data))\n",
    "print('메일의 평균 길이 : %f' % (sum(map(len, X_data))/len(X_data)))\n",
    "max_len = 300\n",
    "# 전체 데이터셋의 길이는 max_len으로 맞춥니다.\n",
    "data = pad_sequences(X_data, maxlen = max_len)\n",
    "print(\"훈련 데이터의 크기(shape): \", data.shape)\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "pred_test = model.predict(data)\n",
    "sample_submission.loc[:,'info'] = np.where(pred_test> 0.5, 1,0).reshape(-1)\n",
    "sample_submission.loc[:,[\"id\",\"info\"]].to_csv(\"submission_RNN.csv\", index = False)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42cNU4ss4X1f"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rqo6cBov4fiA"
   },
   "source": [
    "정확도 : 0.8527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2NjB_5c4jlu"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', \n",
    "                     mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(train_X, train_y, epochs=15, \n",
    "                    callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "#에포크 5까지만 가고 멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJ1d46Cm4pAU"
   },
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "pred_test = loaded_model.predict(test_X)\n",
    "sample_submission.loc[:,'info'] = np.where(pred_test> 0.5, 1,0).reshape(-1)\n",
    "sample_submission.loc[:,[\"id\",\"info\"]].to_csv(\"submission_GRU.csv\", index = False)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co0owK2r4z_R"
   },
   "source": [
    "### 1D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpggWbmf5B9-"
   },
   "source": [
    "임베딩 적용X + Mecab사용\n",
    "정확도 : 0.97277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHUShzmn46Ti"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, \n",
    "Embedding, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(32, 5, strides=1, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDPjYzFu5M2v"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode = 'max', \n",
    "                     verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1TFsq6M5XwA"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, epochs = 10, batch_size=64, \n",
    "                    validation_split=0.2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySnI44Yy5Zrr"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "pred_test = model.predict(test_X)\n",
    "sample_submission.loc[:,'info'] = np.where(pred_test> 0.5, 1,0).reshape(-1)\n",
    "sample_submission.loc[:,[\"id\",\"info\"]].to_csv(\"submission_1DCNN.csv\", index = False)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFHg-DvN5ing"
   },
   "source": [
    "임베딩O + Mecab 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ6GIW0K7bar"
   },
   "source": [
    "정확도 : 0.9724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVqYjrZ55iDi"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, weights = [embedding_matrix], \n",
    "                    input_length = 1000)) # 여기만 바뀜\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(32, 5, strides=1, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qE-rZdf5s4C"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode = 'max', \n",
    "                     verbose = 1, save_best_only = True)\n",
    " \n",
    "history = model.fit(train_X, train_y, epochs = 10, batch_size=64, \n",
    "                    validation_split=0.2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1cmbdbc7OtM"
   },
   "source": [
    "임베딩X + Mecab + 새로운 stopword 사전 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhRERE777h--"
   },
   "source": [
    "정확도 : 0.97072 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjiNJC0j8Bys"
   },
   "outputs": [],
   "source": [
    "stopword = pd.read_csv(\"stopwords.txt\", sep='\\t',names=['word', 'pos', 'val'], header=None)\n",
    "stopword_new = stopword['word'] # 바꿔서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6qIcELl7XJm"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    tokenizer = Mecab()\n",
    "    token_list = []\n",
    "    \n",
    "    for text in tqdm.tqdm(text_list):\n",
    "      try:\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text.lower()) \n",
    "        # 한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
    "        token = tokenizer.morphs(txt) # 형태소 분석\n",
    "        token = [t for t in token if t not in stopword_new or type(t) != float] \n",
    "        #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
    "        token_list.append(token)\n",
    "      except:\n",
    "        token_list.append(None)\n",
    "    return token_list, tokenizer\n",
    "\n",
    "train['new_article'], komoran = text_preprocessing(train['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmpcVAVl8Oyn"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 32))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(32, 5, strides=1, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs = 10, batch_size=64, validation_split=0.2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kryTg4mO-0Mc"
   },
   "source": [
    "## +임계값 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRPRnUv2-_bz"
   },
   "source": [
    "정확도 : 0.97277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cARMzUe-36P"
   },
   "outputs": [],
   "source": [
    "# 성능 지표 출력 함수\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    " \n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    " \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print(\"오차 행렬\")\n",
    "    print(confusion)\n",
    "    print(\"정확도: {0:.5f}, 정밀도: {1:.5f}, 재현율: {2:.5f}, F1: {3:.5f}, AUC: {4:.5f}\"\n",
    "    .format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDwEnuCM_DnQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    " \n",
    "# 평가지표를 조사하기 위한 새로운 함수 생성\n",
    "def get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n",
    "    #thresholds list 객체 내의 값을 iteration 하면서 평가 수행\n",
    "    for custom_threshold in thresholds:\n",
    "        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1)\n",
    "        custom_predict = binarizer.transform(pred_proba_c1)\n",
    "        print('\\n임계값: ', custom_threshold)\n",
    "        get_clf_eval(y_test, custom_predict, pred_proba_c1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hJ4woQN_I_N"
   },
   "outputs": [],
   "source": [
    "# 임계값\n",
    "thresholds = [0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56]\n",
    "pred_proba = model.predict(train_X)\n",
    "get_eval_by_threshold(train_y, pred_proba[:,0].reshape(-1, 1), thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7g_3IBQ_OIA"
   },
   "source": [
    "임계값이 0.55일때 최고라 판정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ryu7GPyP_Jsy"
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(test_X)\n",
    "test_notnull.loc[:,'info'] = np.where(pred_test> 0.55, 1,0).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH0GX64d9Qi2"
   },
   "source": [
    "# 최종모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGY22jzG9l2-"
   },
   "source": [
    "기본 stopwords + Komoran + text2sequence + word2vec + 1D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzmETjKP-Evr"
   },
   "source": [
    "정확도 : 0.97297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xTjweCV9Vmq"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import re\n",
    "import tqdm\n",
    "komoran  = Komoran()\n",
    " \n",
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "    tokenizer = Komoran()\n",
    "    token_list = []\n",
    "    \n",
    "    for text in tqdm.tqdm(text_list):\n",
    "      try:\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text.lower()) \n",
    "        # 한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
    "        token = tokenizer.morphs(txt) # 형태소 분석\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float] \n",
    "        #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
    "        token_list.append(token)\n",
    "      except:\n",
    "        token_list.append(None)\n",
    "    return token_list, tokenizer\n",
    " \n",
    "train['new_article'], komoran = text_preprocessing(train['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4k8wHF7V-WWc"
   },
   "outputs": [],
   "source": [
    "# train으로 학습시킨 형태소 분석기 test에 적용\n",
    "stopwords = ['을', '를', '이', '가', '은', '는', 'null']\n",
    "token_list2 = []\n",
    "for text in tqdm.tqdm(test['content']):\n",
    "  try:\n",
    "    txt2 = re.sub('[^가-힣a-z]', ' ', text.lower())\n",
    "    token2 = komoran.morphs(txt2)\n",
    "    token2 = [t for t in token2 if t not in stopwords or type(t) != float]\n",
    "    token_list2.append(token2)\n",
    "  except:\n",
    "    token_list2.append(None)\n",
    "test['new_article'] = token_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ij-YQgT7-Zut"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_notnull = train[train['new_article'].notnull()]\n",
    " \n",
    "def text2sequence(train_text, max_len=1000):\n",
    "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
    "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
    "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
    "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
    "    print('vocab_size : ', vocab_size)\n",
    "    X_train = pad_sequences(train_X_seq, maxlen = max_len) \n",
    "    #설정한 문장의 최대 길이만큼 padding\n",
    "    \n",
    "    return X_train, vocab_size, tokenizer\n",
    " \n",
    "train_y = train_notnull['info']\n",
    "train_X, vocab_size, vectorizer = text2sequence(train_notnull['new_article'], \n",
    "                                                max_len = 100)\n",
    "print(train_X.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uddZVkji-dlL"
   },
   "outputs": [],
   "source": [
    "test_notnull = test[test['new_article'].notnull()]\n",
    "test_X_seq = vectorizer.texts_to_sequences(test_notnull['new_article'])\n",
    "test_X = pad_sequences(test_X_seq, maxlen = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZuhrtU5-fzW"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    " \n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for index, word in enumerate(vectorizer.word_index):\n",
    "    if word in word2vec:\n",
    "        embedding_vector = word2vec[word] \n",
    "        embedding_matrix[index] = embedding_vector \n",
    "    else:\n",
    "        print(\"word2vec에 없는 단어입니다.\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBykaNmV-kCe"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, \n",
    "Embedding, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, weights = [embedding_matrix], input_length = 1000))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(32, 5, strides=1, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    " \n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode = 'max', \n",
    "                     verbose = 1, save_best_only = True)\n",
    "history = model.fit(train_X, train_y, epochs = 10, batch_size=64, \n",
    "                    validation_split=0.2, callbacks=[es, mc])\n",
    " \n",
    "pred_test = model.predict(test_X)\n",
    "test_notnull.loc[:,'info'] = np.where(pred_test> 0.5, 1,0).reshape(-1)\n",
    "test_notnull['id'] = test_notnull['n_id']+\"_\"+test_notnull['ord'].map(str)\n",
    "test_notnull.drop(['n_id', 'ord'], axis='columns', inplace=True)\n",
    " \n",
    "test['id'] = test['n_id']+\"_\"+test['ord'].map(str)\n",
    "test.drop(['n_id', 'ord'], axis='columns', inplace=True)\n",
    "merge = pd.merge(test, test_notnull, how='outer', on='id')\n",
    "merge = merge.loc[:,['id', 'info']]\n",
    "merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzf8PX5m-nMY"
   },
   "outputs": [],
   "source": [
    "# train에서 발견한 결과 적용하여 결측치 채우기\n",
    " \n",
    "merge.fillna(0, inplace=True) # 숫자들은 모두 0\n",
    "merge.at[29937, 'info']=1\n",
    "merge.at[31326, 'info']=1 \n",
    "# test 셋에서 확인 결과 이 두 인덱스의 content만 ‘]]’였음 (train에서 이를 1로 판별)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TXEIHWlkxTKR",
    "6bqp9MpZFIpB",
    "60S_scX1_W7c"
   ],
   "name": "가짜뉴스 분류 알고리즘_code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
